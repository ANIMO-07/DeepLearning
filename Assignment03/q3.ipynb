{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_kernel(dim, depth=1):\n",
    "    arr = np.zeros((depth,dim,dim))\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            for k in range(depth):\n",
    "                arr[k][i][j]= random.gauss(0,math.sqrt(2/(dim*dim*depth)))\n",
    "    return arr\n",
    "\n",
    "\n",
    "def convolve(image, kernel, padding=0, strides=1):\n",
    "    \n",
    "    dim = kernel.shape[1]\n",
    "\n",
    "    if(len(image.shape)==2):\n",
    "        image = image.reshape((1,image.shape[0],image.shape[1]))\n",
    "    if(len(kernel.shape)==2):\n",
    "        kernel = kernel.reshape((1,dim,dim))\n",
    "\n",
    "    depth = kernel.shape[0]\n",
    "    xOutput = int(((image.shape[1] - dim + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((image.shape[2] - dim + 2 * padding) / strides) + 1)\n",
    "\n",
    "    output = np.zeros((xOutput, yOutput))\n",
    "    imagePadded = np.pad(image, ((0,0),(padding,padding),(padding,padding)), 'constant')\n",
    "\n",
    "    # Iterate through image\n",
    "    for y in range(image.shape[2] - dim +1):\n",
    "        if y % strides == 0:\n",
    "            for x in range(image.shape[1] - dim + 1):\n",
    "                if x % strides == 0:\n",
    "                    for z in range(depth):\n",
    "                        output[x, y] += (kernel[z,:,:] * imagePadded[z, x : x + dim, y : y + dim]).sum()\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return 0 if x < 0 else x\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "def cross_entropy(x):\n",
    "    return -np.log(x)\n",
    "\n",
    "\n",
    "def regularized_cross_entropy(layers, lam, x):\n",
    "    loss = cross_entropy(x)\n",
    "    for layer in layers:\n",
    "        loss += lam * (np.linalg.norm(layer.get_weights()) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return 0 if x < 0 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional:\n",
    "    \n",
    "    def __init__(self,name,depth,num_filters=1,size=3,padding=0,stride=1):\n",
    "        self.depth = depth\n",
    "        self.name = name\n",
    "        self.size = size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.out = None\n",
    "        self.num_filters =num_filters\n",
    "        self.ReLU = np.vectorize(ReLU)\n",
    "        self.last_input = None\n",
    "        self.filters = np.zeros((self.num_filters,self.depth,self.size,self.size))\n",
    "        for i in range(num_filters):\n",
    "            self.filters[i]=kaiming_kernel(self.size,self.depth)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self,image):\n",
    "        print(self.name,\"forward done\\n\")\n",
    "        self.depth = image.shape[0]\n",
    "        self.last_input = image\n",
    "        \n",
    "        self.out = np.zeros((self.num_filters, math.floor((image.shape[1] - self.size + 2*self.padding)/self.stride) + 1, math.floor((image.shape[2] - self.size + 2*self.padding)/self.stride) + 1))\n",
    "        \n",
    "        for i in range(self.num_filters):\n",
    "            self.out[i] = convolve(image,self.filters[i],self.padding,self.stride)\n",
    "            \n",
    "        self.out = self.ReLU(self.out)\n",
    "    \n",
    "        return self.out\n",
    "    \n",
    "    def plot_filters(self, n_filters=10):\n",
    "        x = self.filters.shape[1]\n",
    "        fig = plt.figure(figsize=(20,18))\n",
    "        for i in range(n_filters):\n",
    "            for j in range(x):\n",
    "                fig.add_subplot(10,x,i*x + j +1)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.imshow(self.filters[i][j])\n",
    "        plt.show()\n",
    "    \n",
    "    def backward(self, din, learn_rate=0.005):\n",
    "        print(self.name,\"backward done\\n\")\n",
    "        input_dimension = self.last_input.shape[1]          # input dimension\n",
    "                      # back propagate through ReLU\n",
    "        self.ReLU_derivative(din)\n",
    "\n",
    "        dout = np.zeros(self.last_input.shape)              # loss gradient of the input to the convolution operation\n",
    "        dfilt = np.zeros(self.filters.shape)                # loss gradient of filter\n",
    "\n",
    "        for f in range(self.filters.shape[0]):              # loop through all filters\n",
    "            tmp_y = out_y = 0\n",
    "            while tmp_y + self.size <= input_dimension:\n",
    "                tmp_x = out_x = 0\n",
    "                while tmp_x + self.size <= input_dimension:\n",
    "                    patch = self.last_input[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size]\n",
    "                    dfilt[f] += np.sum(din[f, out_y, out_x] * patch, axis=0)\n",
    "                    dout[:, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size] += din[f, out_y, out_x] * self.filters[f]\n",
    "                    tmp_x += self.stride\n",
    "                    out_x += 1\n",
    "                tmp_y += self.stride\n",
    "                out_y += 1\n",
    "        self.filters -= learn_rate * dfilt                  # update filters using SGD\n",
    "        return dout                                         # return the loss gradient for this layer's inputs\n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.reshape(self.filters, -1)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"Group_28/train/ketch/image_0007.jpg\")\n",
    "img = cv2.resize(img,(224,224))\n",
    "img = np.transpose(img, (2,0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:                                              # max pooling layer using pool size equal to 2\n",
    "    def __init__(self, name, stride=2, size=2):\n",
    "        self.name = name\n",
    "        self.last_input = None\n",
    "        self.stride = stride\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, image):\n",
    "        print(self.name,\"forward done\\n\")\n",
    "        self.last_input = image                             # keep track of last input for later backward propagation\n",
    "\n",
    "        num_channels, h_prev, w_prev = image.shape\n",
    "        h = int((h_prev - self.size) / self.stride) + 1     # compute output dimensions after the max pooling\n",
    "        w = int((w_prev - self.size) / self.stride) + 1\n",
    "\n",
    "        downsampled = np.zeros((num_channels, h, w))        # hold the values of the max pooling\n",
    "\n",
    "        for i in range(num_channels):                       # slide the window over every part of the image and\n",
    "            curr_y = out_y = 0                              # take the maximum value at each step\n",
    "            while curr_y + self.size <= h_prev:             # slide the max pooling window vertically across the image\n",
    "                curr_x = out_x = 0\n",
    "                while curr_x + self.size <= w_prev:         # slide the max pooling window horizontally across the image\n",
    "                    patch = image[i, curr_y:curr_y + self.size, curr_x:curr_x + self.size]\n",
    "                    downsampled[i, out_y, out_x] = np.max(patch)       # choose the maximum value within the window\n",
    "                    curr_x += self.stride                              # at each step and store it to the output matrix\n",
    "                    out_x += 1\n",
    "                curr_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return downsampled\n",
    "\n",
    "    def backward(self, din, learning_rate):\n",
    "        print(self.name,\"backward done\\n\")\n",
    "        num_channels, orig_dim, *_ = self.last_input.shape      # gradients are passed through the indices of greatest\n",
    "                                                                # value in the original pooling during the forward step\n",
    "\n",
    "        dout = np.zeros(self.last_input.shape)                  # initialize derivative\n",
    "\n",
    "        for c in range(num_channels):\n",
    "            tmp_y = out_y = 0\n",
    "            while tmp_y + self.size <= orig_dim:\n",
    "                tmp_x = out_x = 0\n",
    "                while tmp_x + self.size <= orig_dim:\n",
    "                    patch = self.last_input[c, tmp_y:tmp_y + self.size, tmp_x:tmp_x + self.size]    # obtain index of largest\n",
    "                    (x, y) = np.unravel_index(np.nanargmax(patch), patch.shape)                     # value in patch\n",
    "                    dout[c, tmp_y + x, tmp_x + y] += din[c, out_y, out_x]\n",
    "                    tmp_x += self.stride\n",
    "                    out_x += 1\n",
    "                tmp_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return dout\n",
    "\n",
    "    def get_weights(self):                          # pooling layers have no weights\n",
    "        return 0\n",
    "\n",
    "\n",
    "class FullyConnected:                               # fully-connected layer\n",
    "    def __init__(self, name, nodes1, nodes2, activation):\n",
    "        self.name = name\n",
    "        self.weights = np.random.normal(0, 1/math.sqrt(nodes1), size=(nodes1, nodes2))\n",
    "        self.biases = np.zeros(nodes2)\n",
    "        self.activation = activation\n",
    "        self.last_input_shape = None\n",
    "        self.last_input = None\n",
    "        self.last_output = None\n",
    "        self.ReLU = np.vectorize(ReLU)\n",
    "        self.ReLU_derivative = np.vectorize(ReLU_derivative)\n",
    "\n",
    "    def forward(self, input):\n",
    "        print(self.name,\"forward done\\n\")\n",
    "        self.last_input_shape = input.shape         # keep track of last input shape before flattening\n",
    "                                                    # for later backward propagation\n",
    "\n",
    "        input = input.flatten()                                 # flatten input\n",
    "\n",
    "        output = np.dot(input, self.weights) + self.biases      # forward propagate\n",
    "                        # apply ReLU activation function\n",
    "        self.ReLU(output)\n",
    "\n",
    "        self.last_input = input                     # keep track of last input and output for later backward propagation\n",
    "        self.last_output = output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, din, learning_rate=0.005):                           # back propagate through ReLU\n",
    "        print(self.name,\"backward done\\n\")\n",
    "        self.ReLU_derivative(din)\n",
    "\n",
    "        self.last_input = np.expand_dims(self.last_input, axis=1)\n",
    "        din = np.expand_dims(din, axis=1)\n",
    "\n",
    "        dw = np.dot(self.last_input, np.transpose(din))           # loss gradient of final dense layer weights\n",
    "        db = np.sum(din, axis=1).reshape(self.biases.shape)       # loss gradient of final dense layer biases\n",
    "\n",
    "        self.weights -= learning_rate * dw                        # update weights and biases\n",
    "        self.biases -= learning_rate * db\n",
    "\n",
    "        dout = np.dot(self.weights, din)\n",
    "        return dout.reshape(self.last_input_shape)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.reshape(self.weights, -1)\n",
    "\n",
    "\n",
    "class Dense:                                        # dense layer with softmax activation\n",
    "    def __init__(self, name, nodes, num_classes):\n",
    "        self.name = name\n",
    "        self.weights = np.random.normal(0, 1/math.sqrt(nodes), size=(nodes, num_classes))\n",
    "        self.biases = np.zeros(num_classes)\n",
    "        self.last_input_shape = None\n",
    "        self.last_input = None\n",
    "        self.last_output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        print(self.name,\"forward done\\n\")\n",
    "        self.last_input_shape = input.shape         # keep track of last input shape before flattening\n",
    "                                                    # for later backward propagation\n",
    "\n",
    "        input = input.flatten()                                 # flatten input\n",
    "\n",
    "        output = np.dot(input, self.weights) + self.biases      # forward propagate\n",
    "\n",
    "        self.last_input = input                     # keep track of last input and output for later backward propagation\n",
    "        self.last_output = output\n",
    "\n",
    "        return softmax(output)\n",
    "\n",
    "    def backward(self, din, learning_rate=0.005):\n",
    "        print(self.name,\"backward done\\n\")\n",
    "        for i, gradient in enumerate(din):\n",
    "            if gradient == 0:                   # the derivative of the loss with respect to the output is nonzero\n",
    "                continue                        # only for the correct class, so skip if the gradient is zero\n",
    "\n",
    "            t_exp = np.exp(self.last_output)                      # gradient of dout[i] with respect to output\n",
    "            dout_dt = -t_exp[i] * t_exp / (np.sum(t_exp) ** 2)\n",
    "            dout_dt[i] = t_exp[i] * (np.sum(t_exp) - t_exp[i]) / (np.sum(t_exp) ** 2)\n",
    "\n",
    "            dt = gradient * dout_dt                               # gradient of loss with respect to output\n",
    "\n",
    "            dout = self.weights @ dt                              # gradient of loss with respect to input\n",
    "\n",
    "            # update weights and biases\n",
    "            self.weights -= learning_rate * (np.transpose(self.last_input[np.newaxis]) @ dt[np.newaxis])\n",
    "            self.biases -= learning_rate * dt\n",
    "\n",
    "            return dout.reshape(self.last_input_shape)            # return the loss gradient for this layer's inputs\n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.reshape(self.weights, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_curve(accuracy_history, val_accuracy_history):\n",
    "    plt.plot(accuracy_history, 'b', linewidth=3.0, label='Training accuracy')\n",
    "    plt.plot(val_accuracy_history, 'r', linewidth=3.0, label='Validation accuracy')\n",
    "    plt.xlabel('Iteration', fontsize=16)\n",
    "    plt.ylabel('Accuracy rate', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.title('Training Accuracy', fontsize=16)\n",
    "    plt.savefig('training_accuracy.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_learning_curve(loss_history):\n",
    "    plt.plot(loss_history, 'b', linewidth=3.0, label='Cross entropy')\n",
    "    plt.xlabel('Iteration', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.title('Learning Curve', fontsize=16)\n",
    "    plt.savefig('learning_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_sample(image, true_label, predicted_label):\n",
    "    plt.imshow(image)\n",
    "    if true_label and predicted_label is not None:\n",
    "        if type(true_label) == 'int':\n",
    "            plt.title('True label: %d, Predicted Label: %d' % (true_label, predicted_label))\n",
    "        else:\n",
    "            plt.title('True label: %s, Predicted Label: %s' % (true_label, predicted_label))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_histogram(layer_name, layer_weights):\n",
    "    plt.hist(layer_weights)\n",
    "    plt.title('Histogram of ' + str(layer_name))\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Number')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def build_model(self):\n",
    "        self.add_layer(Convolutional(name='conv1', num_filters=32,depth = 3))\n",
    "        self.add_layer(Convolutional(name='conv2', num_filters=64, depth = 32))\n",
    "        self.add_layer(Pooling(name='pool1', stride=1, size=2))\n",
    "        self.add_layer(FullyConnected(name='fullyconnected', nodes1=64*219*219, nodes2=128, activation='relu'))\n",
    "        self.add_layer(Dense(name='dense', nodes=218, num_classes=3))\n",
    "\n",
    "    def forward(self, image, plot_feature_maps):                # forward propagate\n",
    "        for layer in self.layers:\n",
    "            if plot_feature_maps:\n",
    "                image = (image * 255)[0, :, :]\n",
    "                plot_sample(image, None, None)\n",
    "            image = layer.forward(image)\n",
    "        return image\n",
    "\n",
    "    def backward(self, gradient, learning_rate):                # backward propagate\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient, learning_rate)\n",
    "\n",
    "    def train(self, dataset, num_epochs, learning_rate, validate, regularization, plot_weights, verbose):\n",
    "        history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            print('\\n--- Epoch {0} ---'.format(epoch))\n",
    "            loss, tmp_loss, num_corr = 0, 0, 0\n",
    "            for i in range(len(dataset['train_images'])):\n",
    "#                 if i % 100 == 99:\n",
    "                accuracy = (num_corr / (i + 1)) * 100       # compute training accuracy and loss up to iteration i\n",
    "                loss = tmp_loss / (i + 1)\n",
    "\n",
    "                history['loss'].append(loss)                # update history\n",
    "                history['accuracy'].append(accuracy)\n",
    "\n",
    "                \n",
    "                if verbose:\n",
    "                    print('[Step %05d/%03d]: Loss %02.3f | Accuracy: %02.3f ' % (i + 1, len(dataset['train_images']), loss, accuracy))\n",
    "                \n",
    "                \n",
    "                \n",
    "                image = dataset['train_images'][i]\n",
    "                label = dataset['train_labels'][i]\n",
    "\n",
    "                tmp_output = self.forward(image, plot_feature_maps=0)       # forward propagation\n",
    "\n",
    "                # compute (regularized) cross-entropy and update loss\n",
    "                tmp_loss += regularized_cross_entropy(self.layers, regularization, tmp_output[label])\n",
    "\n",
    "                if np.argmax(tmp_output) == label:                          # update accuracy\n",
    "                    num_corr += 1\n",
    "\n",
    "                gradient = np.zeros(10)                                     # compute initial gradient\n",
    "                gradient[label] = -1 / tmp_output[label] + np.sum([2 * regularization * np.sum(np.absolute(layer.get_weights())) for layer in self.layers])\n",
    "\n",
    "\n",
    "                self.backward(gradient, learning_rate)                      # backward propagation\n",
    "                \n",
    "                \n",
    "        if validate:\n",
    "            print('Validation\\n')\n",
    "            indices = np.random.permutation(dataset['validation_images'].shape[0])\n",
    "            val_loss, val_accuracy = self.evaluate(\n",
    "                dataset['validation_images'][indices, :],\n",
    "                dataset['validation_labels'][indices],\n",
    "                regularization=0,\n",
    "                plot_correct=0,\n",
    "                plot_missclassified=0,\n",
    "                plot_feature_maps=0,\n",
    "                verbose=0\n",
    "            )\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "            if verbose:\n",
    "                print('[Step %05d]: Loss %02.3f | Accuracy: %02.3f | '\n",
    "                      'Validation Loss %02.3f | Validation Accuracy: %02.3f' %\n",
    "                      (i + 1, loss, accuracy, val_loss, val_accuracy))\n",
    "                        \n",
    "                        \n",
    "        if verbose:\n",
    "            \n",
    "            print('Train Loss: %02.3f' % (history['loss'][-1]))\n",
    "            print('Train Accuracy: %02.3f' % (history['accuracy'][-1]))\n",
    "            plot_learning_curve(history['loss'])\n",
    "            plot_accuracy_curve(history['accuracy'], history['val_accuracy'])\n",
    "\n",
    "        if plot_weights:\n",
    "            for layer in self.layers:\n",
    "                if 'pool' not in layer.name:\n",
    "                    plot_histogram(layer.name, layer.get_weights())\n",
    "\n",
    "    def evaluate(self, X, y, regularization, plot_correct, plot_missclassified, plot_feature_maps, verbose):\n",
    "        loss, num_correct = 0, 0\n",
    "        for i in range(len(X)):\n",
    "            tmp_output = self.forward(X[i], plot_feature_maps)              # forward propagation\n",
    "\n",
    "            # compute cross-entropy update loss\n",
    "            loss += regularized_cross_entropy(self.layers, regularization, tmp_output[y[i]])\n",
    "\n",
    "            prediction = np.argmax(tmp_output)                              # update accuracy\n",
    "            if prediction == y[i]:\n",
    "                num_correct += 1\n",
    "                if plot_correct:                                            # plot correctly classified digit\n",
    "                    image = (X[i] * 255)[0, :, :]\n",
    "                    plot_sample(image, y[i], prediction)\n",
    "                    plot_correct = 1\n",
    "            else:\n",
    "                if plot_missclassified:                                     # plot missclassified digit\n",
    "                    image = (X[i] * 255)[0, :, :]\n",
    "                    plot_sample(image, y[i], prediction)\n",
    "                    plot_missclassified = 1\n",
    "\n",
    "        test_size = len(X)\n",
    "        accuracy = (num_correct / test_size) * 100\n",
    "        loss = loss / test_size\n",
    "        if verbose:\n",
    "            print('Test Loss: %02.3f' % loss)\n",
    "            print('Test Accuracy: %02.3f' % accuracy)\n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training the model ---\n",
      "\n",
      "--- Epoch 1 ---\n",
      "[Step 00001]: Loss 0.000 | Accuracy: 0.000 \n",
      "conv1 for\n",
      "\n",
      "conv2 for\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "model.build_model()\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.05\n",
    "validate = 1\n",
    "regularization = 0\n",
    "verbose = 1\n",
    "plot_weights = 0\n",
    "plot_correct = 0\n",
    "plot_missclassified = 0\n",
    "plot_feature_maps = 0\n",
    "    \n",
    "print('\\n--- Training the model ---')                                   # train model\n",
    "model.train(\n",
    "    dataset,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    validate,\n",
    "    regularization,\n",
    "    plot_weights,\n",
    "    verbose\n",
    ")\n",
    "\n",
    "print('\\n--- Testing the model ---')                                    # test model\n",
    "model.evaluate(\n",
    "    dataset['test_images'],\n",
    "    dataset['test_labels'],\n",
    "    regularization,\n",
    "    plot_correct,\n",
    "    plot_missclassified,\n",
    "    plot_feature_maps,\n",
    "    verbose\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
